{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUvsR-mWBoNS"
   },
   "source": [
    "# Synthetic Text Generation using Large Language Models from Hugging Face <a href=\"https://colab.research.google.com/github/mostly-ai/mostlyai/blob/main/docs/tutorials/synthetic-text-llm/synthetic-text-llm.ipynb\" target=\"_blank\"><img src=\"https://img.shields.io/badge/Open%20in-Colab-blue?logo=google-colab\" alt=\"Run on Colab\"></a>\n",
    "\n",
    "In this notebook, we demonstrate how to synthesize free text columns using pre-trained large language models with billions of parameters from [Hugging Face](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending).\n",
    "\n",
    "The usage of a GPU, with 24GB of memory or more, is strongly recommended for running this tutorial.\n",
    "\n",
    "MOSTLY AI leverages parameter efficient fine-tuning [[1](#refs)], quantization [[2](#refs)], and activation checkpointing [[3](#refs)] to train large language models in a memory efficient manner. Therefore, even if using a single GPU with only 24GB of memory, you should be able to train LLMs with billions of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKRa93uuSqZS"
   },
   "source": [
    "## Synthesize Data via MOSTLY AI"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "%pip install mostlyai  # or: pip install 'mostlyai[local-gpu]'",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# fetch original data\n",
    "df = pd.read_parquet(\"https://github.com/mostly-ai/public-demo-data/raw/dev/headlines/headlines.parquet\")\n",
    "\n",
    "# split into train and holdout sets, we will use the holdout set to evaluate the performance of the generator later in the tutorial\n",
    "tgt = df.sample(frac=0.9, random_state=42)\n",
    "hol = df.drop(tgt.index)\n",
    "tgt[[\"headline\", \"category\"]].head(5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "W4IDNOIyPW7L",
    "outputId": "413106b9-de23-441b-ae27-f6efe45085a6",
    "tags": []
   },
   "source": [
    "from mostlyai.sdk import MostlyAI\n",
    "\n",
    "# initialize SDK\n",
    "mostly = MostlyAI()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# train a generator on the dataset\n",
    "\n",
    "# specify any Hugging Face language model that fits on a single GPU such as Mistral, Llama, Qwen, and so forth\n",
    "huggingface_model = \"Qwen/Qwen2.5-1.5B\"\n",
    "\n",
    "# NOTE: some models are gated and require a HF_TOKEN environment variable to be set\n",
    "# huggingface_model = \"meta-llama/Llama-3.2-1B\"\n",
    "# import os\n",
    "# os.environ[\"HF_TOKEN\"] = \"hf_...\"  # only needed if you want to use Llama or other gated models\n",
    "\n",
    "config = {\n",
    "    \"name\": \"Synthetic Text Tutorial\",\n",
    "    \"tables\": [\n",
    "        {\n",
    "            \"name\": \"headlines\",\n",
    "            \"data\": tgt,\n",
    "            \"tabular_model_configuration\": {\n",
    "                \"max_training_time\": 1,  # the tabular model should anyways finish in less than 1 min\n",
    "            },\n",
    "            \"language_model_configuration\": {\n",
    "                \"max_training_time\": 20,  # we recommend at least ~20 minutes if training on an A10G or similar GPU\n",
    "                \"model\": huggingface_model,\n",
    "            },\n",
    "            \"columns\": [\n",
    "                {\"name\": \"category\", \"model_encoding_type\": \"TABULAR_CATEGORICAL\"},\n",
    "                {\"name\": \"headline\", \"model_encoding_type\": \"LANGUAGE_TEXT\"},\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "# train a generator\n",
    "g_headlines = mostly.train(config=config)\n",
    "\n",
    "# note: the first time this is run, the LLM will be initially downloaded from Hugging Face, which may take some time depending on your connectivity"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# generate a synthetic dataset\n",
    "syn = mostly.generate(generator=g_headlines, size=5000).data()\n",
    "print(f\"Created synthetic data with {syn.shape[0]:,} records and {syn.shape[1]:,} attributes\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgvJ0XoWTHoX"
   },
   "source": [
    "## Explore Synthetic Text\n",
    "\n",
    "Show 10 randomly sampled synthetic records. Note that you can execute the following cell multiple times to see different samples."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "syn.sample(n=10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to 10 randomly sampled original records."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "tgt.sample(n=10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the synthetic data compare to the original data when taking categories into account?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we perform a sanity check to see if the synthetic data is similar to the original data when taking categories into account. We take each headline, and encode it using a sentence transformer. We then perform PCA dimensionality reduction on the resulting embeddings and visualize the first two principal components. If the synthetic data is similar to the original data, we should see that the synthetic data has similar distribution of categories as the original data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# perform PCA dimensionality reduction and visualization\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# get sentence embeddings for both datasets\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# get the 5 most frequent categories\n",
    "value_counts = syn[\"category\"].value_counts()\n",
    "selected_categories = value_counts.head(5).index\n",
    "\n",
    "# filter datasets to only include most frequent categories\n",
    "tgt_head = tgt[tgt[\"category\"].isin(selected_categories)].sample(1000)\n",
    "syn_head = syn[syn[\"category\"].isin(selected_categories)].sample(1000)\n",
    "\n",
    "tgt_features = model.encode(tgt_head[\"headline\"].tolist())\n",
    "syn_features = model.encode(syn_head[\"headline\"].tolist())\n",
    "\n",
    "# apply PCA to both datasets\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "combined = np.vstack((syn_features, tgt_features))\n",
    "pca.fit(combined)  # important: fit PCA on the combined data\n",
    "syn_pca = pca.transform(syn_features)\n",
    "tgt_pca = pca.transform(tgt_features)\n",
    "\n",
    "# plot the PCA results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(selected_categories)))\n",
    "\n",
    "# plot synthetic data\n",
    "for category, color in zip(selected_categories, colors):\n",
    "    mask = syn_head[\"category\"] == category\n",
    "    ax1.scatter(syn_pca[mask, 0], syn_pca[mask, 1], c=[color], label=category, alpha=0.6)\n",
    "ax1.set_title(\"Synthetic Headlines\")\n",
    "ax1.legend()\n",
    "\n",
    "# plot original data\n",
    "for category, color in zip(selected_categories, colors):\n",
    "    mask = tgt_head[\"category\"] == category\n",
    "    ax2.scatter(tgt_pca[mask, 0], tgt_pca[mask, 1], c=[color], label=category, alpha=0.6)\n",
    "ax2.set_title(\"Original Headlines\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.suptitle(f\"PCA visualization of headlines by category\\n ({huggingface_model})\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a classifier on the synthetic data, and evaluate its performance on the real (original) data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "At the beginning of this tutorial, we saved a holdout set of the original data `hol` which we did not use to train the generator. We will now use this holdout set to evaluate the performance of a classifier trained on the synthetic data, and then compare it to the performance of a classifier trained on the original data (without the holdout set) to see how much information is retained in the synthetic data."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# train simple classifier on the synthetic data, and evaluate its performance on the real data holdout\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# embed the synthetically generated data\n",
    "syn_train_features = model.encode(syn_head[\"headline\"].tolist())\n",
    "syn_train_labels = syn_head[\"category\"].tolist()\n",
    "\n",
    "# filter holdout data to only include categories present in synthetic data\n",
    "mask = hol[\"category\"].isin(syn_train_labels)\n",
    "hol_features = model.encode(hol[mask][\"headline\"].tolist())\n",
    "hol_labels = hol[mask][\"category\"].tolist()\n",
    "\n",
    "# train classifier on synthetic data\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf.fit(syn_train_features, syn_train_labels)\n",
    "\n",
    "# evaluate on holdout set\n",
    "hol_pred = clf.predict(hol_features)\n",
    "print(\"Trained on SYNTHETIC data, evaluated on an actual holdout:\")\n",
    "print(classification_report(hol_labels, hol_pred))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# for comparison, we train a simple classifier on the real data and evaluate its performance on the real data holdout\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# embed the real training data\n",
    "tgt_features = model.encode(tgt_head[\"headline\"].tolist())\n",
    "tgt_labels = tgt_head[\"category\"].tolist()\n",
    "\n",
    "# train classifier on real data\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf.fit(tgt_features, tgt_labels)\n",
    "\n",
    "# evaluate on holdout set\n",
    "hol_pred = clf.predict(hol_features)\n",
    "print(\"Trained on ORIGINAL data, evaluated on an actual holdout:\")\n",
    "print(classification_report(hol_labels, hol_pred))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The synthetic data retains a lot of the structure of the original data, and the classifier trained on the synthetic data should perform similarly to the classifier trained on the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQMOiU1Bv6W4"
   },
   "source": [
    "## Conclusion\n",
    "In this tutorial, we demonstrated how to train a large language model to generate synthetic text conditioned on synthetic tabular data using MOSTLY AI's SDK. We analyzed the generated texts and showed that the synthetic data was of similar structure as the original data. We then proceeded to train a classifier on the synthetic data, and evaluated its performance on the original data.\n",
    "\n",
    "This feature allows the user to make use of the world knowledge encoded in large language models to generate synthetic text data, all while retaining the structure of the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further exercises\n",
    "\n",
    "In addition to walking through the above instructions, we suggest..\n",
    "* try seeded generation, condition your LLM on different data\n",
    "* try using a different dataset\n",
    "* try multi-billion parameter models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References<a class=\"anchor\" name=\"refs\"></a>\n",
    "\n",
    "1. LoRA: Low-Rank Adaptation of Large Language Models, https://arxiv.org/abs/2106.09685\n",
    "1. QLoRA: Efficient Finetuning of Quantized LLMs, https://arxiv.org/abs/2305.14314\n",
    "1. Training Deep Nets with Sublinear Memory Cost, https://arxiv.org/abs/1604.06174"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
