{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUvsR-mWBoNS"
   },
   "source": [
    "# Validate synthetic data via Train-Synthetic-Test-Real <a href=\"https://colab.research.google.com/github/mostly-ai/mostlyai/blob/main/docs/tutorials/train-synthetic-test-real/TSTR.ipynb\" target=\"_blank\"><img src=\"https://img.shields.io/badge/Open%20in-Colab-blue?logo=google-colab\" alt=\"Run on Colab\"></a>\n",
    "\n",
    "In this tutorial, we demonstrate the process of evaluating the quality of synthetic data based on its utility for a downstream Machine Learning (ML) task. The method is commonly referred to as the Train-Synthetic-Test-Real (TSTR) evaluation [[1](#refs)]. The TSTR evaluation serves as a robust measure of synthetic data quality because ML models rely on the accurate representation of deeper underlying patterns to perform effectively on previously unseen data. As a result, this approach offers a more reliable assessment than simply evaluating higher-level statistics.\n",
    "\n",
    "See image below for the general setup of TSTR.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mostly-ai/mostly-tutorials/dev/train-synthetic-test-real/TSTR.png' width=\"600px\"/>\n",
    "\n",
    "Thus, we take actual (=real) data, and split it into a holdout and a training dataset. Next, we create a synthetic dataset only based on the training data. Then we train a Machine Learning (ML) model, and do so once using the synthetic data and once using the actual training data. And finally we evaluate the performance of each of those two models on top of the actual holdout data, that was kept aside all along. By comparing the performance of these two models, we can assess how much utility has been retained by the synthesization method with respect to a specific ML task.\n",
    "\n",
    "Note, that one needs to use a true holdout for the evaluation to properly measure out-of-sample performance, as this is the relevant metric for real-world use cases. If one uses the same training data that has been used for the synthesis, one would \"leak\" information from training into evaluation. This becomes particularly an issue for synthesizers that are prone to overfitting, and simply memorize the samples that it has been exposed to. If one, on the other hand, were to use synthetic data for the evaluation, one would not get meaningful results either, as the synthetic data might not be representative of the real data. E.g., consider the degenerate case of a synthesizer that only produces the same record over and over again. Any model trained on that data, would yield perfect results when evaluated on it again, whereas it will be of no use when applied to real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZ7ERZK__8TB"
   },
   "source": [
    "## Synthesize Data via MOSTLY AI\n",
    "\n",
    "For this tutorial, we will be using a cleaned up version of the UCI Adult Income [[2](#refs)] dataset, that itself stems from the 1994 American Community Survey [[3](#refs)] by the US census bureau. The dataset consists of 48,842 records, 14 mixed-type features and has 1 target variable, that indicates whether a respondent had or had not reported a high level of annual income. This dataset is being selected, as it's one of the go-to datasets commonly used to showcase machine learning models in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mostlyai  # or: pip install 'mostlyai[local]'\n",
    "%pip install scikit-learn seaborn lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T16:43:44.736651Z",
     "start_time": "2024-03-04T16:41:21.187721Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# fetch original data\n",
    "df = pd.read_csv(\"https://github.com/mostly-ai/public-demo-data/raw/dev/census/census.csv.gz\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split into training and validation\n",
    "df_trn, df_hol = train_test_split(df, test_size=0.2, random_state=1)\n",
    "\n",
    "print(f\"training data with {df_trn.shape[0]:,} records and {df_trn.shape[1]} attributes\")\n",
    "print(f\"holdout data with {df_hol.shape[0]:,} records and {df_hol.shape[1]} attributes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mostlyai.sdk import MostlyAI\n",
    "\n",
    "# initialize SDK\n",
    "mostly = MostlyAI()\n",
    "\n",
    "# train a generator on the original training data\n",
    "g = mostly.train(data=df_trn, name=\"TSTR Tutorial Census\")\n",
    "\n",
    "# probe the generator for synthetic data\n",
    "syn = mostly.probe(g, size=len(df))\n",
    "print(f\"Created synthetic data with {syn.shape[0]:,} records and {syn.shape[1]:,} attributes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Synthetic Data\n",
    "\n",
    "Show 10 randomly sampled synthetic records. Note, that you can execute the following cell multiple times, to see different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T16:46:03.515106Z",
     "start_time": "2024-03-04T16:46:03.485987Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "syn.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show 5 randomly sampled Female Professors of age 30 or younger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T16:46:05.959842Z",
     "start_time": "2024-03-04T16:46:05.944168Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "syn.loc[(syn[\"sex\"] == \"Female\") & (syn[\"education\"] == \"Prof-school\") & (syn[\"age\"] <= 30)].sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count low-income (<=50K) and high-income (>50K) records within the synhetic sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T16:46:07.425237Z",
     "start_time": "2024-03-04T16:46:07.415740Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "syn[\"income\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Count low-income and high-income records among the group of non-US citizen, that have been divorced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T16:46:09.020547Z",
     "start_time": "2024-03-04T16:46:09.006756Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "syn.loc[(syn[\"native_country\"] != \"United-States\") & (syn[\"marital_status\"] == \"Divorced\")][\"income\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgvJ0XoWTHoX"
   },
   "source": [
    "## Compare ML Performance\n",
    "\n",
    "Let's now train a state-of-the-art **LightGBM** classifier on top of the synthetic data, to then check how well it can predict whether an actual person reported an annual income of more than $50K or not. We will then compare the predictive accuracy to a model, that has been trained on the actual data, and see whether we were able to achieve a similar performance purely based on the synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T16:46:10.767340Z",
     "start_time": "2024-03-04T16:46:10.715898Z"
    },
    "id": "Rl6-YXB_e0Ac",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 72\n",
    "\n",
    "target_col = \"income\"\n",
    "target_val = \">50K\"\n",
    "\n",
    "\n",
    "def prepare_xy(df):\n",
    "    y = (df[target_col] == target_val).astype(int)\n",
    "    str_cols = [col for col in df.select_dtypes([\"object\", \"string\"]).columns if col != target_col]\n",
    "    for col in str_cols:\n",
    "        df[col] = pd.Categorical(df[col])\n",
    "    cat_cols = [col for col in df.select_dtypes(\"category\").columns if col != target_col]\n",
    "    num_cols = [col for col in df.select_dtypes(\"number\").columns if col != target_col]\n",
    "    for col in num_cols:\n",
    "        df[col] = df[col].astype(\"float\")\n",
    "    X = df[cat_cols + num_cols]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def train_model(X, y):\n",
    "    cat_cols = list(X.select_dtypes(\"category\").columns)\n",
    "    X_trn, X_val, y_trn, y_val = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    ds_trn = lgb.Dataset(X_trn, label=y_trn, categorical_feature=cat_cols, free_raw_data=False)\n",
    "    ds_val = lgb.Dataset(X_val, label=y_val, categorical_feature=cat_cols, free_raw_data=False)\n",
    "    model = lgb.train(\n",
    "        params={\"verbose\": -1, \"metric\": \"auc\", \"objective\": \"binary\"},\n",
    "        train_set=ds_trn,\n",
    "        valid_sets=[ds_val],\n",
    "        callbacks=[early_stopping(5)],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, hol):\n",
    "    X_hol, y_hol = prepare_xy(hol)\n",
    "    probs = model.predict(X_hol)\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    auc = roc_auc_score(y_hol, probs)\n",
    "    acc = accuracy_score(y_hol, preds)\n",
    "    probs_df = pd.concat(\n",
    "        [\n",
    "            pd.Series(probs, name=\"probability\").reset_index(drop=True),\n",
    "            pd.Series(y_hol, name=target_col).reset_index(drop=True),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    sns.displot(data=probs_df, x=\"probability\", hue=target_col, bins=20, multiple=\"stack\")\n",
    "    plt.title(f\"Accuracy: {acc:.1%}, AUC: {auc:.1%}\", fontsize=20)\n",
    "    plt.show()\n",
    "    return auc\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDK1Jr_xhBnl"
   },
   "source": [
    "### Train a Model on Synthetic Data - Test on Real Data\n",
    "\n",
    "We train the LightGBM on synthetic data, and then evaluate its performance on holdout data. We report two performance metrics: \n",
    "1. **Accuracy**: This is the probability to correctly predict the `income` class of a randomly selected record.\n",
    "2. **AUC** (Area-Under-Curve): This is the probability to correctly predict the `income` class, if two records, one of high-income and one of low-income are given.\n",
    "\n",
    "Whereas the Accuracy informs about the overall ability to get the class attribution correct, the AUC specifically informs about the ability to properly rank records, with respect to their probability of being within the target class or not. In both cases, the higher the metric, the better the predictive accuracy of the model.\n",
    "\n",
    "The displayed chart shows the distribution of scores, that the model assigned to each of the holdout records. A score close to 0 means that model is very confident, that the record is of low income. A score close to 1 means that the model is very confident that it's a high income record. These scores are further split by their actual outcome, i.e. whether they are or are not actually high income. This allows to visually inspect the model's confidence in assigning the right scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14CMIhcvgQ77",
    "outputId": "f8f46533-b092-4ec7-8ecd-c2f5c51961ec",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare synthetic data, and split into features `X` and target `y`\n",
    "X_syn, y_syn = prepare_xy(syn)\n",
    "# train ML model on synthetic data with early stopping to prevent overfitting\n",
    "model_syn = train_model(X_syn, y_syn)\n",
    "# evaluate trained model on original holdout data\n",
    "auc_syn = evaluate_model(model_syn, df_hol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlKk4PLmB-0b"
   },
   "source": [
    "### Train a Model on Real Data - Test on Real Data\n",
    "\n",
    "Let's now compare these results achieved on synthetic data, with a model trained on real data. For a very good synthesizer, we expect to see a predictive performance of the two models being close to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZtQiGZdyCB72",
    "outputId": "fb0fa7ad-c3f3-4d8a-d907-a7248e770e6d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare original training data, and split into features `X` and target `y`\n",
    "X_trn, y_trn = prepare_xy(df_trn)\n",
    "# train ML model on original training data with early stopping to prevent overfitting\n",
    "model_trn = train_model(X_trn, y_trn)\n",
    "# evaluate trained model on original holdout data\n",
    "auc_trn = evaluate_model(model_trn, df_hol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMGNussThvys"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "For the given dataset, and the given synthesizer, we can observe a near on-par performance of the synthetic data with respect to the given downstream ML task. This means, that one can train the model purely on synthetic data, and yield just as good results as if it were trained on real data, but without ever putting the privacy of any of the contained individuals at any risk.\n",
    "\n",
    "## Further exercises\n",
    "\n",
    "In addition to walking through the above instructions, we suggest..\n",
    "* to run Train-Synthetic-Test-Real \n",
    "  * using a different dataset, eg. the UCI bank-marketing dataset [[4](#refs)]\n",
    "  * using a different downstream ML model, eg. a RandomForest model [[5](#refs)]\n",
    "  * using a different synthesizer, eg. SynthCity, SDV, etc.\n",
    "* to check the impact of synthetic upsampling\n",
    "  * generate 10x or 100x the original data records, and see whether it improves ML accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References<a class=\"anchor\" name=\"refs\"></a>\n",
    "\n",
    "1. https://arxiv.org/pdf/1706.02633.pdf §3.1.2\n",
    "1. https://archive.ics.uci.edu/ml/datasets/adult\n",
    "1. https://www.census.gov/programs-surveys/acs\n",
    "1. https://archive.ics.uci.edu/ml/datasets/bank+marketing\n",
    "1. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
