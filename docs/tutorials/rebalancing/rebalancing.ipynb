{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUvsR-mWBoNS"
   },
   "source": [
    "# Explore the Benefits of Rebalancing\n",
    "\n",
    "In this exercise, we are going to explore the benefits of synthetic rebalancing of heavily imbalanced datasets, where a minority class of interest accounts for less than 0.1% of cases. \n",
    "\n",
    "Rebalancing can be useful for cases where we want to learn more of an otherwise small or underrepresented population segment by seeing more examples thereof. Of course, also a synthesizer can only leverage the data that it has seen. But if the method is data-efficient, and in particular more effective than the downstream data consumer, then it is possible to gain a significant advantage by synthetic rebalancing.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mostly-ai/mostly-tutorials/dev/rebalancing/rebalancing.png' width=\"600px\"/>\n",
    "\n",
    "In terms of evaluation, we again turn towards the Train-Synthetic-Test-Real approach to benchmark the predictive accuracy of a model that is trained on the (rebalanced) synthetic data, and compare that to a model trained on the (imbalanced) actual data. In addition, we will also benchmark against established methods for rebalancing, like naive upsampling as well as SMOTE. All four models are then evaluated on a holdout data, and compared in terms of predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZ7ERZK__8TB",
    "tags": []
   },
   "source": [
    "## Synthesize Data via MOSTLY AI\n",
    "\n",
    "For this tutorial, we will be using again the UCI Adult Income [[1](#refs)] dataset, as well as the same training and validation split, that was used in the Train-Synthetic-Test-Real tutorial. However, we will create an artificial imbalance of 0.1% of high-income records in the training data, by downsampling the minority class.\n",
    "\n",
    "The code below will automatically create a rebalanced synthetic dataset using the MOSTLY AI Synthetic Data SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U 'mostlyai[local]'\n",
    "#!pip install scikit-learn seaborn lightgbm imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# fetch original data\n",
    "df = pd.read_csv(\"https://github.com/mostly-ai/public-demo-data/raw/dev/census/census.csv.gz\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split into training and validation\n",
    "df_trn, df_hol = train_test_split(df, test_size=0.2, random_state=1)\n",
    "\n",
    "print(f\"training data with {df_trn.shape[0]:,} records and {df_trn.shape[1]} attributes\")\n",
    "print(f\"holdout data with {df_hol.shape[0]:,} records and {df_hol.shape[1]} attributes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an artificial imbalance of 0.1% of high-income records in the training data, by downsampling the minority class\n",
    "\n",
    "\n",
    "def create_imbalance(df, target, ratio):\n",
    "    val_min, val_maj = df[target].value_counts().sort_values().index\n",
    "    df_maj = df.loc[df[target] == val_maj]\n",
    "    n_min = int(df_maj.shape[0] / (1 - ratio) * ratio)\n",
    "    df_min = df.loc[df[target] == val_min].sample(n=n_min, random_state=1)\n",
    "    df_maj = df.loc[df[target] == val_maj]\n",
    "    df_imb = pd.concat([df_min, df_maj]).sample(frac=1, random_state=1)\n",
    "    return df_imb\n",
    "\n",
    "\n",
    "trn = create_imbalance(df_trn, \"income\", 1 / 1000)\n",
    "print(f\"Created imbalanced training data with {trn.shape[0]:,} records and {trn.shape[1]} attributes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mostlyai.sdk import MostlyAI\n",
    "\n",
    "# initialize SDK\n",
    "mostly = MostlyAI(local=True)  # or: MostlyAI(base_url='xxx', api_key='xxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a generator on the original training data\n",
    "g = mostly.train(\n",
    "    config={\n",
    "        \"name\": \"Rebalancing Tutorial Census\",\n",
    "        \"tables\": [\n",
    "            {\n",
    "                \"name\": \"data\",\n",
    "                \"data\": trn,\n",
    "                \"tabular_model_configuration\": {\n",
    "                    \"max_training_time\": 5,\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a synthetic dataset with rebalancing of the income column to 50% \">50K\" category\n",
    "sd = mostly.generate(\n",
    "    generator=g,\n",
    "    config={\n",
    "        \"name\": \"Rebalancing Tutorial Census\",\n",
    "        \"tables\": [\n",
    "            {\"name\": \"data\", \"configuration\": {\"rebalancing\": {\"column\": \"income\", \"probabilities\": {\">50K\": 0.5}}}}\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "\n",
    "# start using it\n",
    "syn = sd.data()\n",
    "print(f\"Created synthetic data with {syn.shape[0]:,} records and {syn.shape[1]:,} attributes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples Random Records\n",
    "\n",
    "Let's first show 10 randomly sampled original records, from the imbalanced dataset. Try executing the cell multiple times, to see different samples. Still, due to the strong imbalance, you will hardly ever encounter a sample of the high income class (i.e. `income` being `>50K`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trn.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now display 10 randomly sampled synthetic records. Again, run the cell multiple times. This time, you should see that the records are evenly distributed across the two `income` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "syn.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Female Doctors with a High Income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now investigate all female doctors with a high income. But, it turns out there are actually none in the original data, thus we won't be able to learn anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trn[(trn[\"income\"] == \">50K\") & (trn.sex == \"Female\") & (trn.education == \"Doctorate\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the synthetic data does contain a list of realistic, statistically sound female doctors with a high income, that allow to learn about this particular subsegment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "syn[(syn[\"income\"] == \">50K\") & (syn.sex == \"Female\") & (syn.education == \"Doctorate\")].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare ML Performance via TSTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 72\n",
    "\n",
    "target_col = \"income\"\n",
    "target_val = \">50K\"\n",
    "\n",
    "\n",
    "def prepare_xy(df: pd.DataFrame):\n",
    "    y = (df[target_col] == target_val).astype(int)\n",
    "    str_cols = [col for col in df.select_dtypes([\"object\", \"string\"]).columns if col != target_col]\n",
    "    for col in str_cols:\n",
    "        df[col] = pd.Categorical(df[col])\n",
    "    cat_cols = [col for col in df.select_dtypes(\"category\").columns if col != target_col]\n",
    "    num_cols = [col for col in df.select_dtypes(\"number\").columns if col != target_col]\n",
    "    for col in num_cols:\n",
    "        df[col] = df[col].astype(\"float\")\n",
    "    X = df[cat_cols + num_cols]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def train_model(X, y):\n",
    "    cat_cols = list(X.select_dtypes(\"category\").columns)\n",
    "    X_trn, X_val, y_trn, y_val = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    ds_trn = lgb.Dataset(X_trn, label=y_trn, categorical_feature=cat_cols, free_raw_data=False)\n",
    "    ds_val = lgb.Dataset(X_val, label=y_val, categorical_feature=cat_cols, free_raw_data=False)\n",
    "    model = lgb.train(\n",
    "        params={\"verbose\": -1, \"metric\": \"auc\", \"objective\": \"binary\"},\n",
    "        train_set=ds_trn,\n",
    "        valid_sets=[ds_val],\n",
    "        callbacks=[early_stopping(5)],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, hol):\n",
    "    X_hol, y_hol = prepare_xy(hol)\n",
    "    probs = model.predict(X_hol)\n",
    "    auc = roc_auc_score(y_hol, probs)\n",
    "    f1 = f1_score(y_hol, probs > 0.5, average=\"macro\")\n",
    "    probs_df = pd.concat(\n",
    "        [\n",
    "            pd.Series(probs, name=\"probability\").reset_index(drop=True),\n",
    "            pd.Series(y_hol, name=target_col).reset_index(drop=True),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    sns.displot(data=probs_df, x=\"probability\", hue=target_col, bins=20, multiple=\"stack\")\n",
    "    plt.title(f\"AUC: {auc:.1%}, F1 Score: {f1:.2f}\", fontsize=20)\n",
    "    plt.show()\n",
    "    return auc\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_hol_min = df_hol.loc[df_hol[\"income\"] == \">50K\"]\n",
    "print(\n",
    "    f\"Holdout data consists of {df_hol.shape[0]:,} records\",\n",
    "    f\"with {df_hol_min.shape[0]:,} samples from the minority class\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model on the original imbalanced training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_trn, y_trn = prepare_xy(trn)\n",
    "model_trn = train_model(X_trn, y_trn)\n",
    "auc_trn = evaluate_model(model_trn, df_hol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an AUC of about 60%, the model trained on the imbalanced dataset is just as good as a flip of a coin. I.e., the downstream LightGBM model is not able to learn any signal due to the low number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train model on naively rebalanced training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "X_trn, y_trn = prepare_xy(trn)\n",
    "sm = RandomOverSampler(random_state=1)\n",
    "X_trn_up, y_trn_up = sm.fit_resample(X_trn, y_trn)\n",
    "model_trn_up = train_model(X_trn_up, y_trn_up)\n",
    "auc_trn_up = evaluate_model(model_trn_up, df_hol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random \"naive\" upsampling [[2](#refs)], which simply adds minority samples multiple times to achieve a balance, only marginally helps the downstream model in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train model on SMOTE rebalanced training data\n",
    "\n",
    "SMOTE upsampling [[3](#refs)], which creates novel (non-privacy-preserving) samples by interpolating between neighboring samples, does boost the performance of the downstream model to close to 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "X_trn, y_trn = prepare_xy(trn)\n",
    "categorical_mask = (X_trn.dtypes == \"category\").tolist()\n",
    "categorical_features_indices = [i for i, is_categorical in enumerate(categorical_mask) if is_categorical]\n",
    "sm = SMOTENC(categorical_features=categorical_features_indices, random_state=1)\n",
    "X_trn_smote, y_trn_smote = sm.fit_resample(X_trn, y_trn)\n",
    "model_trn_smote = train_model(X_trn_smote, y_trn_smote)\n",
    "auc_trn_smote = evaluate_model(model_trn_smote, df_hol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model on balanced synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_syn, y_syn = prepare_xy(syn)\n",
    "model_syn = train_model(X_syn, y_syn)\n",
    "auc_syn = evaluate_model(model_syn, df_hol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both, performance measures, the AUC [[4](#refs)] as well as the macro-averaged F1 score [[5](#refs)] are significantly better for the model that was trained on synthetic data, than if it were trained on any of the other methods. This is a strong proof of value of synthetic rebalancing for learning more about a small sub-group within the population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMGNussThvys"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "For the given dataset and the given synthesizer we can see, that both data analysts as well as AI engineers can learn more from a balanced synthetic dataset when compared to the imbalanced original dataset. Note, that the actual lift in performance may vary, depending on the dataset, the predictive task, and the chosen ML model.\n",
    "\n",
    "## Further exercises\n",
    "\n",
    "In addition to walking through the above instructions, we suggest..\n",
    "* to repeat the experiments for different class imbalances - see the helper script at the bottom to create such experiments\n",
    "* to repeat the experiments for different datasets, ML models, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References<a class=\"anchor\" name=\"refs\"></a>\n",
    "\n",
    "1. https://archive.ics.uci.edu/ml/datasets/adult\n",
    "1. https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html\n",
    "1. https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTENC.html\n",
    "1. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
    "1. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
